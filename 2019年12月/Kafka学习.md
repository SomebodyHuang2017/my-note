# Kafka学习

## 生产者消息分区机制原理剖析

### 概述
Kafka有主题（Topic）的概念，他是承载真实数据的逻辑容器。在主题下面还分为若干个分区，也就是说kafka的消息组织方式其实是一个三级结构：主题 - 分区 - 消息。主题下的消息只会保存在某一个分区中，而不会在多个分区被保存多份。

### 分区的作用是什么？
分区的作用就是提供负载均衡能力，为了实现系统的高伸缩性，不同的分区能放在不同的机器上，每个节点的机器都能独立地执行各自分区的读写请求处理。并且我们可以增加节点机器来增加系统的吞吐量。

Kafka中叫分区，MongoDB和Elasticsearch中就叫分片Shard，而在HBase中则叫Region，在Cassandra中称为vnode。他们的叫法不同，但对底层分区（partition）的整体思想却不尽相同。

### 常见的分区策略有哪些？

首先要了解什么是分区策略？所谓的分区策略就是决定生产者将消息发送到哪个分区的算法。

Kafka默认提供了一些分区策略，也支持自定义分区策略，只需要实现 org.apache.kafka.clients.producer.Partitioner接口。通常也只需要实现partition方法。

常见的分区策略：
1. 轮询策略（Round-robin策略）：顺序分配，这也是API默认提供的分区策略。轮询策略有非常优秀的负载均衡表现，它总是能保证消息最大限度地被平均分配到所有分区上，故默认情况下它是最合理的分区策略，也是我们最常用的分区策略之一。
2. 随机策略（Randomness策略）：随机分配，如果追求平均分配，还是使用轮询策略比较好。
3. 按消息键保存策略（Key-ordering策略）：kafka允许为每条消息定义消息键，简称为key。这个key可以表示业务含义，也啃一用来表征消息元数据。
4. 其他分区策略：用户自定义分区策略，比如按照地域划分的策略

## 生产者压缩算法面面观

### 概述
压缩（compression），它秉承了用时间去换空间的经典 trade-off 思想，具体来说就是用CPU时间去换磁盘空间或网络I/O传输量，希望用较小的CPU开销带来更少的磁盘占用或更少的网络I/O传输。

### 怎么压缩？
目前Kafka有两大类消息格式，社区目前称之为V1版本和V2版本。V2版本是Kafka 0.11.0.0中正式引入的。

不论是哪个版本，Kafka的消息层次都分为两层：消息集合（message set）以及消息（message）。一个消息集合包含若干条日志想（record item），而日志项才是真正封装消息都地方。Kafka底层的消息日志由一些列消息集合日志项组成。Kafka通常不会直接操作具体的一条条消息，他重视在消息集合这个层面上进行写处理。

### 何时压缩？
在Kafka中，压缩可能发生在两个地方：生产者端和Broker端。

生产者程序中配置 `compression.type` 参数即表示启用制定类型的压缩算法。Broker端一般不会进行压缩，但是有两种情况可能出现会读消息重新压缩：
* Broker端制定了和Producer端不一样的压缩算法。一般Broker的配置是`compression.type=producer`，但是有可能这个配置被人为修改了，这回造成Broker端的CPU使用率飙升，所以最好不要在Broker上修改这个配置。
* Broker端发生了消息格式转换。一般这种情况是为了兼容老版本的消费者程序，Broker才会对消息进行解压缩和重新压缩，这种情况下对性能影响还是很大，也会失去零拷贝这个特性。

### 何时解压缩？
通常情况下解压缩发生在consumer端，也就是说Producer发送压缩消息到Broker后，Broker照单全收不做处理，Consumer请求这部分消息的时候Broker依然原样发出去，消息到达Consumer后，由Consumer自行进行解压缩还原消息。压缩算法携带在消息集合中，Consumer直接取出就能清楚Producer使用了压缩算法了。

总结就是：**Producer端压缩、Broker保持、Consumer端解压缩。**

### 各种压缩算法对比
Kafka 2.1.0版本之前，支持三种压缩算法：GZIP、Snappy和LZ4，从2.1.0开始，Kafka正式支持Zstandard算法（zstd）。它是Facebook开源的一个压缩算法，能够提供超高的压缩比。
吞吐量：LZ4 > Snappy > zstd & GZIP
压缩比：zstd > LZ4 > GZIP > Snappy

### 最佳实践
因为压缩会消耗CPU资源，所以只要当producer的CPU资源比较充足，就可以开启压缩。减少传输带宽的压力，也能减少磁盘空间的占用。

## 无消息丢失配置怎么实现？

### 概述
首先要清楚Kafka到底在什么情况下才能保证消息不丢失？

**一句话概括，Kafka只对“已提交”的消息（committed message）做有限度的持久化保证。**

第一个核心要素是**“已提交的消息”**。它指的是若干个Broker成功地接收到一条消息并写入到日志文件后，它会告诉生产者程序这条消息已成功提交。此时在kafka看来就是正式变为“已提交”消息了。为什么是若干个Broker呢？这取决于你对已提交的定义，可以选择一个Broker成功保存改消息就是已提交，还是所有Broker都保存才算已提交。不论哪种情况，Kafka只对已提交的消息做持久化保证这件事是不变的。
第二个核心要素就是**“做有限度的持久化保证”**，也就是说kafka不能保证任何情况下都不丢失，比如极端情况机器损坏，地球不存在了。

### 案例1:生产者丢失数据
目前Kafka Producer是异步发送消息的，例如使用producer.send(msg)这个API去发送消息，它会立即返回，但是不能人为消息发送已成功完成。有很多因素会导致消息没有发送成功，比如网络抖动、消息不合格导致Broker拒绝（比如消息太大，超过Broker的承载能力）等。

所以说这就不能算是Kafka的“锅”，为了解决这样的问题，很好办。不要使用这个API就行了。**使用producer.send(msg,callback)。**不要小瞧这个callback（回调），这个回调能准确地告诉你消息是否提交成功，如果存在提交失败的情况，我们还可以有针对的进行处理。

### 案例2: 消费者程序丢失数据
Consumer端丢失数据主要体现在Consumer端要消费的消息不见了，Consumer程序有个“位移”的概念，他表示Consumer当前消费到 Topic 分区的位置，它类似于书签。Kafka中Consumer端的消息丢失就是怎么一回事。要对抗这种消息丢失，办法很简单：**维持先消费消息，再更新位移的顺序**即可。这样就能最大限度的保证消息不丢失，但是这样也有可能带来消息被重复消费的现象，怎么处理之后会说。

### 最佳实践
1. 不要使用 producer.send(msg)，要使用带有通知回调的 producer.send(msg, callback)
2. 设置 acks = all，代表所有副本Broker都要接收到消息，改消息才算是“已提交”
3. 设置 retries 为一个较大的值
4. 设置 unclean.leader.election.enable = false，它控制了哪些Broker有资格竞选分区的Leader，落后的不允许竞选
5. 设置 replication.factory >= 3，表示消息多保存几份，做冗余
6. 设置 min.insync.replicas > 1，控制消息至少要被写入到多少个副本才算是“已提交”，设置成大于1可以提升消息持久性，实际生产中千万不要使用默认值1
7. 确保 replication.factor > min.insync.replicas 如果两者相等，那么只要有一个副本挂机，整个分区就无法正常工作，我们不仅要改善消息的持久性，防止数据丢失，还要在不降低可用性的基础上完成。推荐设置成 replication.factor = min.insync.replicas + 1
8. 确保消息消费完成再提交，Consumer端有个参数 enable.auto.commit，最好把它设置成false，并采用手动提交唯一的方式

## 客户端都有哪些不常见但是很高级的功能？
Kafka拦截器

### 什么是拦截器？
就像Spring Interceptor 一样，基本思想就是允许应用程序在不修逻辑的情况下，动态地实现一组可插拔的事件处理逻辑链。他能沟在祝业务逻辑的前后多个时间点上插入对应的拦截器逻辑。Kafka的蓝机器借鉴了这个思路，比如在消息返送前或者在消息被消费后。拦截器是在Kafka 0.11.0.0版本被引入的。

### Kafka拦截器
**Kafka拦截器分为生产者拦截器和消费者拦截器。**生产者拦截器允许你在发送消息前以及消息提交成功后植入你的拦截器逻辑；而消费者拦截器支持在消费消息前以及提交位移后编写特定逻辑。

举个例子，假设想再产生消息前执行两个“前置动作”：第一个是为了消息增加一个头信息，封装发送该消息的时间，第二个是更新发送消息数字段，那么你讲这两个拦截器串联在一起统一指定给Producer后，Producer会按顺序执行上面的动作，然后再发送消息。

当前Kafka拦截器的设置方法是通过参数配置完成的。生产者和消费者两端有一个相同的参数，名字叫 `interceptor.classes`，它指定的是一组类的列表，每个类就是特定逻辑的拦截器实现类。

编写所有的生产者拦截器都要继承`org.apache.kafka.clients.producer.ProducerInterceptor`接口。该接口有两个核心方法。
1. onSend：该方法会在消息发送之前被调用。
2. onAcknowledgement：该方法会在消息成功提交或发送失败之后被调用。

编写所有的消费者拦截器都要继承`org.apache.kafka.clients.consumer.ConsumerInterceptor`接口。该接口同样有两个核心方法。
1. onConsumer：该方法在消息返回给Consumer程序之前调用。也就是说在开始处理消息之前，拦截器会先拦一道，搞一些事情，之后再返回给你。
2. onCommit：Consumer在提交位移之后调用该方法。通常你可以在该方法中做一些记账类的动作，比如打日志等。

### 典型使用场景

Kafka拦截器都能用在哪些地方呢？其实很多拦截器的用法相同，**Kafka拦截器可以以应用于包括客户端监控、端到端系统性能检测、消息审计等多种功能在内的场景。**，比如可以统计消息端到端处理的延时。

## Java生产者是如何管理TCP连接的？
### 为何采用TCP?
TCP本身提供了一些高级的功能，比如多路复用（multiplexing request）请求以及同时轮询多个连接的能力。

### Kafka 生产者程序概览
Kafka的Java生产者API主要的对象就是KafkaProducer。通常我们开发一个生产者的步骤有4步。
* 第1步：构造生产者对象所需的参数对象。
* 第2步：利用第1步的参数对象，创建KafkaProducer对象实例。
* 第3步：使用KafkaProducer的send方法发送消息。
* 第4步：调用KafkaProducer的close方法关闭生产者并释放各种系统资源。

### 何时创建TCP连接？
**在创建KafkaProducer实例时，生产者应用会在后台创建并启动一个名为Sender的线程，该Sender线程开始运行时首先会创建与Broker的连接。** 它连接的是`bootstrap.servers`参数指定的所有Broker。

它只会在这个时候创建TCP连接吗？当然不是！**TCP 连接还可能在两个地方被创建：一个是在更新元数据后，另一个是在消息发送时。**

### 何时关闭TCP连接？
Producer端关闭TCP连接的方式有两种：**一种是用户主动关闭；一种是Kafka自动关闭。**
主动关闭一般指的是producer.close()，当然还包括手动kill -9。
Kafka自动关闭是与producer端设置了参数`connections.max.idle.ms`有关。默认情况下该参数值是9分钟，即如果在9分钟内没有任何请求“流过”某个TCP连接，那么Kafka会主动帮你把连接关闭。用户可以在Producer端设置connections.max.idle.ms = -1禁掉这种机制，但是这属于被动关闭（Producer主动发起的连接，却被Broker关闭了），这会导致产生大量的CLOSE_WAIT连接。

